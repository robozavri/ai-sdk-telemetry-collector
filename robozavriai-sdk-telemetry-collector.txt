Current State: The package v1.0.2 works, but it relies on manual instrumentation. You have to explicitly call a function (like sendCustomTelemetry or a direct fetch) inside the 
onFinish
 callback of the Vercel AI SDK functions to send telemetry data.
The Goal: You want the package to perform automatic instrumentation. It should automatically capture all relevant data from the Vercel AI SDK (prompts, responses, token usage, performance, etc.) without requiring manual calls in your application code.
Technical Gap: The package currently doesn't correctly or fully hook into the Vercel AI SDK's experimental_telemetry system or use other mechanisms to automatically intercept the data. The enableAISDKIntegration: true flag in your code seems to be a feature you want rather than one that is fully implemented and working.
Prompt for AI Assistant
Here is a detailed prompt you can provide to another AI assistant to guide it in updating the @robozavri/ai-sdk-telemetry-collector package.

Subject: Upgrade @robozavri/ai-sdk-telemetry-collector for Automatic Vercel AI SDK Instrumentation
Objective:

Your task is to modify the @robozavri/ai-sdk-telemetry-collector TypeScript package to automatically capture and send telemetry data from the Vercel AI SDK. The goal is to eliminate the need for manual data collection in user code. The final solution should be simple, reliable, and thoroughly tested.

Core Requirements:

Automatic Tracking: The package must automatically capture all telemetry from Vercel AI SDK functions like streamText, generateText, streamObject, etc. This includes:
Prompts: The full input messages/prompt.
Responses: The complete, final AI-generated response.
Token Usage: promptTokens, completionTokens, and totalTokens.
Performance Metrics: Time to first chunk, total generation time (msToFinish).
Model Information: The model name and provider.
Finish Reason: e.g., stop, length, tool-calls.
Tool Calls: If any tools are used, capture the tool call details and the tool outputs.
Metadata: Any custom metadata passed by the user.
Reliable & Simple Code:
The implementation must be robust and handle edge cases gracefully (e.g., API errors, streaming interruptions).
It should be simple for a developer to enable. Ideally, they initialize the collector, enable it, and it just works.
The code must be well-structured, following modern TypeScript best practices and be easily testable.
Technical Implementation Guide:

You should implement a "wrapper" or "patching" mechanism to intercept calls to the Vercel AI SDK.

Option 1: The Wrapper Approach (Preferred)

Create a Wrapper Function: Create a function, let's call it wrapWithTelemetry, that takes a Vercel AI SDK function (e.g., streamText) as input and returns a new, instrumented version of it.
Instrument the Call: The wrapped function should:
Record the startTime before calling the original AI SDK function.
Inject an 
onFinish
 callback (or augment an existing one) to capture the results.
Inside 
onFinish
, gather all required data (usage, text, finishReason, etc.).
Calculate performance metrics (e.g., responseTime = Date.now() - startTime).
Format the data into the telemetry event schema.
Use the AITelemetryCollector's internal sender to dispatch the data.
Crucially, the wrapper must pass all original arguments through and return the original function's output (StreamingTextResponse, Promise, etc.) transparently.
Example Usage for the End-User:
import { streamText } from 'ai';
import { openai } from '@ai-sdk/openai';
import { telemetry } from './telemetry-collector'; // Your package

// The user wraps the AI SDK function once
const instrumentedStreamText = telemetry.wrap(streamText);

// And uses it normally everywhere else
export async function POST(req: Request) {
  const { messages } = await req.json();
  const result = await instrumentedStreamText({
    model: openai('gpt-4o'),
    messages,
  });
  return result.toDataStreamResponse();
}

Option 2: The Patching/Monkey-Patching Approach

Identify Target Functions: Identify the core functions in the Vercel AI SDK that need to be patched (e.g., streamText from the ai package).
Apply the Patch: When the collector is enabled, it should dynamically replace the original AI SDK functions with your wrapped versions. This is more "magical" but can be more brittle if the AI SDK's internal structure changes.
What to Avoid:

Do not rely on experimental_telemetry: This feature is not guaranteed to provide all the necessary data (like the full prompt/response content) and may change. Your solution should be self-contained.
Do not require manual calls: The entire point of this upgrade is to make tracking automatic. The user should not have to write collector.send(...) in their own code.
Deliverables:

Updated Package Code: The modified TypeScript source code for the @robozavri/ai-sdk-telemetry-collector package implementing the automatic instrumentation.
New Unit/Integration Tests: Add tests using a framework like Vitest or Jest to verify:
That the wrapper correctly captures all telemetry fields.
That the original functionality of the wrapped AI SDK function remains unchanged.
That it works with both streaming and non-streaming functions.
Updated 
README.md
: The documentation must be updated with clear instructions on how to use the new automatic tracking feature. Provide a simple, copy-pasteable example.
A clean, reliable, and tested implementation that is ready for publishing.


Many packages track telemetry without needing a wrapper. It is possible to do something like this so that for example the installer only needs 2 lines of code for the configuration.
example:
import { AITelemetryCollector } from '@robozavri/ai-sdk-telemetry-collector';
const collector = new AITelemetryCollector({
Â  serverUrl: 'http://localhost:3001/analytics/telemetry',
Â  apiKey: process.env.TELEMETRY_API_KEY || 'test-key',
Â  debug: true, // Enable debug logging
})
collector.enable();

Â const result = await streamText({
Â  Â  Â  model: openai('gpt-4o') as any,
Â  Â  Â  messages: coreMessages,
Â  Â  Â  experimental_telemetry: {
Â  Â  Â  Â  isEnabled: true,
Â  Â  Â  Â  functionId: 'chat-assistant',
Â  Â  Â  Â  metadata: {
Â  Â  Â  Â  Â  userId: 'test-user',
Â  Â  Â  Â  Â  sessionId: sessionId,
Â  Â  Â  Â  Â  requestId: requestId,
Â  Â  Â  Â  },
Â  Â  Â  },
Â  Â  Â  onFinish: async (result) => {
Â  Â  Â  Â  console.log('ðŸ“Š Telemetry automatically collected by AISDKIntegration v1.0.2');
Â  Â  Â  },
Â  Â  });